{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/FoKB5Z5.png\" align=\"left\" width=\"300\" height=\"250\" title=\"source: imgur.com\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Code: J620-002-4:2020 \n",
    "\n",
    "## Program Name: FRONT-END SOFTWARE DEVELOPMENT\n",
    "\n",
    "## Title : P43 - Introduction Ensemble Method - Bagging & Boosting\n",
    "\n",
    "#### Name: \n",
    "\n",
    "#### IC Number:\n",
    "\n",
    "#### Date :\n",
    "\n",
    "#### Introduction : \n",
    "\n",
    "\n",
    "\n",
    "#### Conclusion :\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble method : Bagging and boosting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we will look at the concept of ensemble learning. We will cover primarily bagging and boosting models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are ensemble methods?\n",
    "\n",
    "Ensemble learning is a machine learning method where many models are trained to solve the same problem and combined to produce better results. The key theory is that we can obtain more reliable and/or robust models when weak models are properly combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Weak Learner\n",
    "In the field of machine learning, regardless of whether we have a problem with classification or regression, model selection is extremely important in order to achieve good results. This choice can depend on many of the variables of the problem: amount of data, dimensionality of space, hypothesis of distribution ...\n",
    "A low preference and a low variance, while differing most frequently in opposite directions, are the two most critical features a model expects. In fact, to \"solve\" a problem, we would like our model to have ample freedom to solve the underlying complexities of the data we use, but also less freedom to avoid high variances and to become more robust. This is the well-known tradeoff between bias and variance.\n",
    "<a href=\"https://imgur.com/3G9XZg0\"><img src=\"https://i.imgur.com/3G9XZg0.png\" title=\"source: imgur.com\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regard to ensemble learning, we call weak learners (or simple models) models that can be used as building blocks by combining many of them in order to construct more complex models. Such simple models most often don't perform as well either because they have a strong bias (for instance, a low degree of freedom models) or because they have too much variance to be robust (for instance , a high degree of freedom model). Then, the aim of ensemble approaches is to try to reduce the bias and/or variance of these poor learners by adding many together, in order to obtain better results for a strong learner (or ensemble model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Weak learners\n",
    "\n",
    "We must first select our basic models to be aggregated in order to construct an ensemble learning process. Most of the time (including in the well known bagging and boosting methods) a single base learning algorithm is used so that we have homogeneous weak learners that are trained in different ways. We then state that the ensemble configuration we obtain is \"homogeneous.\" Nonetheless, some methods still exist which use different types of basic learning algorithms: some heterogeneous weak students are then combined into a \"heterogeneous model of groups.\"\n",
    "One important point is that our choice of weak learners should be coherent with the way we aggregate these models. If we choose base models with low bias but high variance, it is with an aggregation procedure that aims to minimize variance, while if we choose base models with low variance but high bias, it is with an aggregation approach that is usually bias reduction.\n",
    "This takes us to the issue of how these models can be integrated. Two main types of meta-algorithms that combine weak learners can be mentioned:\n",
    "1. bagging, that often considers homogeneous weak learners, learns them independently from each other in parallel and combines them following some kind of deterministic averaging process\n",
    "2. boosting, that often considers homogeneous weak learners, learns them sequentially in a very adaptative way (a base model depends on the previous ones) and combines them following a deterministic strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weak learners can be combined to get a model with better performances. The way to combine base models should be adapted to their types. Low bias and high variance weak models should be combined in a way that makes the strong model more robust whereas low variance and high bias base models better be combined in a way that makes the ensemble model less biased. This is shown in Figure 2\n",
    "\n",
    "<a href=\"https://imgur.com/Om3jkBN\"><img src=\"https://i.imgur.com/Om3jkBN.png\" title=\"source: imgur.com\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "In parallel methods we fit the different considered learners independently from each others and, so, it is possible to train them concurrently. The most famous such approach is “bagging” (standing for “bootstrap aggregating”) that aims at producing an ensemble model that is more robust than the individual models composing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "Let’s begin by defining bootstrapping. This statistical technique consists in generating samples of size B (called bootstrap samples) from an initial dataset of size N by randomly drawing with replacement B observations.This is shown Figure 3\n",
    "\n",
    "<a href=\"https://imgur.com/v359my9\"><img src=\"https://i.imgur.com/v359my9.png\" title=\"source: imgur.com\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under certain assumptions, these samples have very strong statistical properties; they can be seen in the first approximation as drawing both directly from the real (and often unknown) distribution of data and independently of each other. They can therefore be regarded as descriptive and independent data distribution samples (almost i.e. samples). The hypothesis which must be confirmed in order to make this approximation true is double. Second , in order to capture much of the variance of the underlying distribution, the size N of the original dataset should be high enough that the sampling from a dataset is a good approximation of the actual distribution sample (representativeness). Second, in contrast to size B of bootstrap samples, the size N of the dataset should be enough so that samples are not excessively correlated (independence). Note that we often refer to these characteristics (representativity and independence) of Bootstrap samples in the following: readers should always note that this is just an approximation.\n",
    "\n",
    "Bootstrap samples are also used for the estimation of statistical estimators variances or confidence intervals, for example. By definition, some observations are based on a statistical estimator and thus a random variable with variation. We have to test this variance with multiple independent samples from the distribution of interest to estimate the variance of such an estimator. In most instances, it will take way too much data to find genuinely unbiased samples as opposed to the amount currently available. We can then use bootstrapping to produce different bootstrap samples, which are \"almost representative\" and \"almost independent\" (almost i.e. samples). Such samples of bootstrap help us to estimate the variance of the estimator by evaluating its value. \n",
    "\n",
    "<a href=\"https://imgur.com/HkhEnJX\"><img src=\"https://i.imgur.com/HkhEnJX.png\" title=\"source: imgur.com\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping is often used to evaluate variance or confidence interval of some statistical learners, as shown in Figure 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Bagging model\n",
    "\n",
    "When training a model, no matter if we are dealing with a classification or a regression problem, we obtain a function that takes an input, returns an output and that is defined with respect to the training dataset. Due to the theoretical variance of the training dataset (we remind that a dataset is an observed sample coming from a true unknown underlying distribution), the fitted model is also subject to variability: if another dataset had been observed, we would have obtained a different model.\n",
    "The idea of bagging is then simple: we want to fit several independent models and “average” their predictions in order to obtain a model with a lower variance. However, we can’t, in practice, fit fully independent models because it would require too much data. So, we rely on the good “approximate properties” of bootstrap samples (representativity and independence) to fit models that are almost independent.\n",
    "\n",
    "First, we create multiple bootstrap samples so that each new bootstrap sample will act as another (almost) independent dataset drawn from true distribution. Then, we can fit a weak learner for each of these samples and finally aggregate them such that we kind of “average” their outputs and, so, obtain an ensemble model with less variance that its components. Roughly speaking, as the bootstrap samples are approximatively independent and identically distributed (i.i.d.), so are the learned base models. Then, “averaging” weak learners outputs do not change the expected answer but reduce its variance (just like averaging i.i.d. random variables preserve expected value but reduce variance).\n",
    "\n",
    "So, assuming that we have L bootstrap samples (approximations of L independent datasets) of size B denoted\n",
    "\n",
    "<a href=\"https://imgur.com/khHP4tO\"><img src=\"https://i.imgur.com/khHP4tO.png\" title=\"source: imgur.com\" /></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can fit L almost independent weak learners (one on each dataset)\n",
    "<a href=\"https://imgur.com/a5aeBr5\"><img src=\"https://i.imgur.com/a5aeBr5.png\" title=\"source: imgur.com\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then aggregate them into some kind of averaging process in order to get an ensemble model with a lower variance. For example, we can define our strong model such that\n",
    "<a href=\"https://imgur.com/bVASBnU\"><img src=\"https://i.imgur.com/bVASBnU.png\" title=\"source: imgur.com\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several potential ways to incorporate parallel fitted multiple versions. For a regression problem, the outputs of and variable can be summed literally to obtain the output of the ensemble model. The class generated by each model is considered to be a vote and the class receiving the bulk of the votes is replaced by the ensemble model (this is known as hard-voting). Even if we are still dealing with a classification problem, we can also consider the probabilities of each class that all models return, average the probabilities and retain the class with the highest average probability. Simple or weighted percentages or votes may be used if appropriate weights can be used.\n",
    "Ultimately, one of the main benefits of bagging is that it can be paralleled. Since the different models are individually fitted, intensive parallelization techniques can be used if necessary. As a summary, we can see how bagging fits in several base models based on different bootstrap samples and build an ensemble model that averages the results of these weak learners.\n",
    "\n",
    "<a href=\"https://imgur.com/q3gsDKU\"><img src=\"https://i.imgur.com/q3gsDKU.png\" title=\"source: imgur.com\" /></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n",
      "Accuracy: 0.857 (0.035)\n"
     ]
    }
   ],
   "source": [
    "#Sample Bagging code\n",
    "# test classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=5)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)\n",
    "# evaluate bagging algorithm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=5)\n",
    "# define the model\n",
    "model = BaggingClassifier()\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 1\n"
     ]
    }
   ],
   "source": [
    "#Predicting an outcome using bagging\n",
    "# make predictions using bagging for classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=5)\n",
    "# define the model\n",
    "model = BaggingClassifier()\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[-4.7705504,-1.88685058,-0.96057964,2.53850317,-6.5843005,3.45711663,-7.46225013,2.01338213,-0.45086384,-1.89314931,-2.90675203,-0.21214568,-0.9623956,3.93862591,0.06276375,0.33964269,4.0835676,1.31423977,-2.17983117,3.1047287]]\n",
    "yhat = model.predict(row)\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n",
      "MAE: -100.174 (10.383)\n"
     ]
    }
   ],
   "source": [
    "#Bagging for regression\n",
    "# test regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=5)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)\n",
    "# evaluate bagging ensemble for regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=5)\n",
    "# define the model\n",
    "model = BaggingRegressor()\n",
    "# evaluate the model\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: -198\n"
     ]
    }
   ],
   "source": [
    "#Making a prediction on the regression example\n",
    "# bagging ensemble for making predictions for regression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=5)\n",
    "# define the model\n",
    "model = BaggingRegressor()\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[0.88950817,-0.93540416,0.08392824,0.26438806,-0.52828711,-1.21102238,-0.4499934,1.47392391,-0.19737726,-0.22252503,0.02307668,0.26953276,0.03572757,-0.51606983,-0.39937452,1.8121736,-0.00775917,-0.02514283,-0.76089365,1.58692212]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to explore other classifiers besides a decision tree? Let's try with a k-nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.884 (0.036)\n"
     ]
    }
   ],
   "source": [
    "# evaluate bagging with knn algorithm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=5)\n",
    "# define the model\n",
    "model = BaggingClassifier(base_estimator=KNeighborsClassifier())\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return to Random Forest\n",
    "\n",
    "Learning tree models for ensemble methods are very common. Strong students consisting of several trees can be called \"forests.\" Trees that make up a forest can be either shallow (few depths) or deep (a lot of depth, if not fully cultivated). Shallow trees have less variation than higher bias and are best chosen for the sequential methods listed below. On the other side, deep trees have low but high variance, so the bagging method is necessary to minimize variance.\n",
    "The random approach to forests is a bagging system where deep trees are combined with bootstraps to achieve a lower production. Random forests use another trick, however, to make multiple fitted trees a little less associated with each other: we also sample characteristics and hold only a random sub-set of them to create the tree, in lieu of only sampling the observations in the dataset to produce a bootstrap samples.\n",
    "\n",
    "Sampling features simply means that not all trees look at exactly the same data for their decisions and thereby minimize the similarity between the different outputs returned. Another advantage of sample compared to features is that it makes the decision-making process more resilient to missing data: findings (from the training dataset or not) with missing data that still be returned or categorized based on trees that only take features into account when data is not missing. Whereas random forestry algorithms combine bagging principles and random subspace feature selection in order to generate more robust models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://imgur.com/TiqOtCv\"><img src=\"https://i.imgur.com/TiqOtCv.png\" title=\"source: imgur.com\" /></a>\n",
    "\n",
    "Random forest method is a bagging method with trees as weak learners. Each tree is fitted on a bootstrap sample considering only a subset of variables randomly chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "Boosting methods work in the same spirit as bagging methods: we construct a family of models to obtain a strong and better learner. In comparison to bagging which mainly aims to minimize variance, boosting is a technique that consists of sequentially adapting several poor learners: each model in the sequence is equipped with more focus on observations in data sets which have been badly treated in the sequence by previous models. Every new model intuitively focuses its attention on the most challenging observations it has made to match up to now, so that at the end of the cycle a good learner with a lower bias is reached (though we can see that boosting can also lead to a reduction of variance). Boosting can be used for both regression and classification problems, including bagging.\n",
    "\n",
    "The basic models that are mostly considered for boosting are models that are based primarily on reducing biases but with small yet high variance. For example, we would typically choose shallow decision-making trees with only a few depths if we want to use trees as our model basis. Another main explanation why low variance but high bias models are used as poor learners to improve is that these models are usually less costly to suit computationally (fast degrees of freedom if parametrised). Indeed, since calculation for the various models can not be conducted in parallel (unlike bagging), many complex models may become too costly to fit sequentially.\n",
    "\n",
    "If poor learners are selected, we also need to determine how they are sequentially configured (what knowledge do we take into account for previous models if fitting the current one?) and how they are added (how do we integrate the current model in the previous one?). In the following paragraphs, we will discuss these issues, describing in particular two major boosting algorithms: adaboost and gradient boost.\n",
    "\n",
    "In brief, these two meta-algorithms differ in how weak learners are created and aggregated during the sequential process. Adaptive boost updates the weight of each observation of the training data set, while gradient boosting updates the value. This major difference stems from the manner in which both approaches attempt to solve the problem of designing the best model to be written as a weighted sum of poor learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting consists in, iteratively, fitting a weak learner, aggregate it to the ensemble model and “update” the training dataset to better take into account the strengths and weakness of the current ensemble model when fitting the next base model, as shown in Figure 10.\n",
    "\n",
    "<a href=\"https://imgur.com/OsylHaa\"><img src=\"https://i.imgur.com/OsylHaa.png\" title=\"source: imgur.com\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive boosting\n",
    "\n",
    "In adaptative boosting (often called “adaboost”), we try to define our ensemble model as a weighted sum of L weak learners\n",
    "<a href=\"https://imgur.com/yIArji2\"><img src=\"https://i.imgur.com/yIArji2.png\" title=\"source: imgur.com\" /></a>\n",
    "Finding the best ensemble model with this form is a difficult optimisation problem. Then, instead of trying to solve it in one single shot (finding all the coefficients and weak learners that give the best overall additive model), we make use of an iterative optimisation process that is much more tractable, even if it can lead to a sub-optimal solution. More especially, we add the weak learners one by one, looking at each iteration for the best possible pair (coefficient, weak learner) to add to the current ensemble model. In other words, we define recurrently the (s_l)’s such that\n",
    "<a href=\"https://imgur.com/nO6gaOO\"><img src=\"https://i.imgur.com/nO6gaOO.png\" title=\"source: imgur.com\" /></a>\n",
    "where c_l and w_l are chosen such that s_l is the model that fit the best the training data and, so, that is the best possible improvement over s_(l-1). We can then denote\n",
    "<a href=\"https://imgur.com/ZU5tKKH\"><img src=\"https://i.imgur.com/ZU5tKKH.png\" title=\"source: imgur.com\" /></a>\n",
    "where E(.) is the fitting error of the given model and e(.,.) is the loss/error function. Thus, instead of optimising “globally” over all the L models in the sum, we approximate the optimum by optimising “locally” building and adding the weak learners to the strong model one by one.\n",
    "More especially, when considering a binary classification, we can show that the adaboost algorithm can be re-written into a process that proceeds as follow. First, it updates the observations weights in the dataset and train a new weak learner with a special focus given to the observations misclassified by the current ensemble model. Second, it adds the weak learner to the weighted sum according to an update coefficient that expresse the performances of this weak model: the better a weak learner performs, the more it contributes to the strong learner.\n",
    "So, assume that we are facing a binary classification problem, with N observations in our dataset and we want to use adaboost algorithm with a given family of weak models. At the very beginning of the algorithm (first model of the sequence), all the observations have the same weights 1/N. Then, we repeat L times (for the L learners in the sequence) the following steps:\n",
    "fit the best possible weak model with the current observations weights\n",
    "compute the value of the update coefficient that is some kind of scalar evaluation metric of the weak learner that indicates how much this weak learner should be taken into account into the ensemble model\n",
    "update the strong learner by adding the new weak learner multiplied by its update coefficient\n",
    "compute new observations weights that expresse which observations we would like to focus on at the next iteration (weights of observations wrongly predicted by the aggregated model increase and weights of the correctly predicted observations decrease)\n",
    "Repeating these steps, we have then build sequentially our L models and aggregate them into a simple linear combination weighted by coefficients expressing the performance of each learner. Notice that there exists variants of the initial adaboost algorithm such that LogitBoost (classification) or L2Boost (regression) that mainly differ by their choice of loss function.Adaboost updates weights of the observations at each iteration. Weights of well classified observations decrease relatively to weights of misclassified observations. Models that perform better have higher weights in the final ensemble model. This shown in Figure 14\n",
    "<a href=\"https://imgur.com/XMJ7C2b\"><img src=\"https://i.imgur.com/XMJ7C2b.png\" title=\"source: imgur.com\" /></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\python-dscourse\\lib\\site-packages\\sklearn\\model_selection\\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.760457963089542\n"
     ]
    }
   ],
   "source": [
    "#adaboost code\n",
    "# AdaBoost Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 30\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting\n",
    "In gradient boosting, the ensemble model we try to build is also a weighted sum of weak learners\n",
    "<a href=\"https://imgur.com/yIArji2\"><img src=\"https://i.imgur.com/yIArji2.png\" title=\"source: imgur.com\" /></a>\n",
    "Just as we mentioned for adaboost, finding the optimal model under this form is too difficult and an iterative approach is required. The main difference with adaptative boosting is in the definition of the sequential optimisation process. Indeed, gradient boosting casts the problem into a gradient descent one: at each iteration we fit a weak learner to the opposite of the gradient of the current fitting error with respect to the current ensemble model. Let’s try to clarify this last point. First, theoretical gradient descent process over the ensemble model can be written\n",
    "<a href=\"https://imgur.com/nO6gaOO\"><img src=\"https://i.imgur.com/nO6gaOO.png\" title=\"source: imgur.com\" /></a>\n",
    "where E(.) is the fitting error of the given model, c_l is a coefficient corresponding to the step size and\n",
    "<a href=\"https://imgur.com/R7FCl8l\"><img src=\"https://i.imgur.com/R7FCl8l.png\" title=\"source: imgur.com\" /></a>\n",
    "is the opposite of the fitting error gradient with respect to the ensemble model at phase l-1. This (most abstract) opposite of the gradient is a function which can only be evaluated in practice for measurements in the training data set (for which we know inputs and outputs): these tests are referred to as pseudo-residuals associated to each observation. Moreover, while we know the values of those pseudo-residuals for observations, we don't want to add a function to our ensemble model: we only want to add a new instance of a weak model. So, the natural thing to do is to suit a weak pseudo-residual student, which is measured for every observation. Finally, a one-dimensional optimization procedure is used to measure the coefficient c l (line check for best phase size c l).\n",
    "Suppose we would like to use gradient boosting techniques for a certain family of poor models. The pseudo-residuals at the start of the algorithm (first sequence model) are set equal to the observational values. Then, we repeat the following steps in L cycles (for series L models):\n",
    "\n",
    "1. Fit the best possible weak pseudo residual pseudo-learner (approximately opposite to the present strong pseudo-learner)\n",
    "\n",
    "2. Calculate the appropriate phase size value by how much we update the ensemble model for the new weak learner\n",
    "\n",
    "3. Update the layout of the ensemble by introducing a new weak learner multiplied by the step size (make a step downward)\n",
    "\n",
    "4. Compute new pseudo-residuals showing, for every observation, the direction in which the ensemble model predictions are next modified.\n",
    "\n",
    "If these steps are repeated, we have sequentially built our L models and added them to a gradient descent method. Note that, while Adaptive boosting tries to resolve the \"local\" optimization problem exactly during each iteration (find the best weak student and his coefficient for adding to the strong model), the gradient boosting approach instead uses a gradient descent approach and can be adapted more easily to many loss functions. Gradient boost can therefore be considered as an adaboost generalization of arbitrary differentiable loss functions.\n",
    "\n",
    "Gradient boosting updates values of the observations at each iteration. Weak learners are trained to fit the pseudo-residuals that indicate in which direction to correct the current ensemble model predictions to lower the error. This is shown in Figure 18\n",
    "<a href=\"https://imgur.com/MxjjWhl\"><img src=\"https://i.imgur.com/MxjjWhl.png\" title=\"source: imgur.com\" /></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.915 (0.027)\n",
      "Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "#Gradient boosting\n",
    "\n",
    "# gradient boosting for classification in scikit-learn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# evaluate the model\n",
    "model = GradientBoostingClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "# fit the model on the whole dataset\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data.\n",
    "\n",
    "XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.\n",
    "\n",
    "XGBoost stands for eXtreme Gradient Boosting.\n",
    "\n",
    "The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms. Which is the reason why many people use xgboost.\n",
    "\n",
    "It is an implementation of gradient boosting machines created by Tianqi Chen, now with contributions from many developers. It belongs to a broader collection of tools under the umbrella of the Distributed Machine Learning Community or DMLC who are also the creators of the popular mxnet deep learning library.\n",
    "\n",
    "Tianqi Chen provides a brief and interesting back story on the creation of XGBoost in the post Story and Lessons Behind the Evolution of XGBoost.\n",
    "\n",
    "XGBoost is a software library that you can download and install on your machine, then access from a variety of interfaces. Specifically, XGBoost supports the following main interfaces:\n",
    "\n",
    "Command Line Interface (CLI).\n",
    "C++ (the language in which the library is written).\n",
    "Python interface as well as a model in scikit-learn.\n",
    "R interface as well as a model in the caret package.\n",
    "Julia.\n",
    "Java and JVM languages like Scala and platforms like Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library is laser focused on computational speed and model performance, as such there are few frills. Nevertheless, it does offer a number of advanced features.\n",
    "\n",
    "**Model Features**\n",
    "The implementation of the model supports the features of the scikit-learn and R implementations, with new additions like regularization. Three main forms of gradient boosting are supported:\n",
    "\n",
    "Gradient Boosting algorithm also called gradient boosting machine including the learning rate.\n",
    "Stochastic Gradient Boosting with sub-sampling at the row, column and column per split levels.\n",
    "Regularized Gradient Boosting with both L1 and L2 regularization.\n",
    "\n",
    "**System Features**\n",
    "The library provides a system for use in a range of computing environments, not least:\n",
    "\n",
    "Parallelization of tree construction using all of your CPU cores during training.\n",
    "Distributed Computing for training very large models using a cluster of machines.\n",
    "Out-of-Core Computing for very large datasets that don’t fit into memory.\n",
    "Cache Optimization of data structures and algorithm to make best use of hardware.\n",
    "\n",
    "**Algorithm Features**\n",
    "The implementation of the algorithm was engineered for efficiency of compute time and memory resources. A design goal was to make the best use of available resources to train the model. Some key algorithm implementation features include:\n",
    "\n",
    "Sparse Aware implementation with automatic handling of missing data values.\n",
    "Block Structure to support the parallelization of tree construction.\n",
    "Continued Training so that you can further boost an already fitted model on new data.\n",
    "XGBoost is free open source software available for use under the permissive Apache-2 license.\n",
    "\n",
    "**Why Use XGBoost?**\n",
    "The two reasons to use XGBoost are also the two goals of the project:\n",
    "\n",
    "1. Execution Speed.\n",
    "2. Model Performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. XGBoost Execution Speed**\n",
    "Generally, XGBoost is fast. Really fast when compared to other implementations of gradient boosting.\n",
    "\n",
    "Szilard Pafka performed some objective benchmarks comparing the performance of XGBoost to other implementations of gradient boosting and bagged decision trees. He wrote up his results in May 2015 in the blog post titled “Benchmarking Random Forest Implementations“.\n",
    "\n",
    "He also provides all the code on GitHub and a more extensive report of results with hard numbers.\n",
    "\n",
    "Figure 19 shows the speed comparison related to XGBoost\n",
    "\n",
    "<a href=\"https://imgur.com/JK75KXR\"><img src=\"https://i.imgur.com/JK75KXR.png\" title=\"source: imgur.com\" /></a>\n",
    "\n",
    "**2. XGBoost Model Performance**\n",
    "XGBoost dominates structured or tabular datasets on classification and regression predictive modeling problems.\n",
    "\n",
    "The evidence is that it is the go-to algorithm for competition winners on the Kaggle competitive data science platform.\n",
    "\n",
    "For example, there is an incomplete list of first, second and third place competition winners that used titled: XGBoost: Machine Learning Challenge Winning Solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Algorithm Does XGBoost Use?**\n",
    "\n",
    "The XGBoost library implements the gradient boosting decision tree algorithm.\n",
    "\n",
    "This algorithm goes by lots of different names such as gradient boosting, multiple additive regression trees, stochastic gradient boosting or gradient boosting machines.\n",
    "\n",
    "Boosting is an ensemble technique where new models are added to correct the errors made by existing models. Models are added sequentially until no further improvements can be made. A popular example is the AdaBoost algorithm that weights data points that are hard to predict.\n",
    "\n",
    "Gradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.\n",
    "\n",
    "This approach supports both regression and classification predictive modeling problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing XGBoost**\n",
    "\n",
    "In Anaconda, the command is conda install -c anaconda py-xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Regression example\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xgboost regression example\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.Series(boston.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    reg_lambda=1,\n",
    "    gamma=0,\n",
    "    max_depth=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=3,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020938</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.04041</td>\n",
       "      <td>0.050353</td>\n",
       "      <td>0.197422</td>\n",
       "      <td>0.012369</td>\n",
       "      <td>0.050169</td>\n",
       "      <td>0.01311</td>\n",
       "      <td>0.039311</td>\n",
       "      <td>0.084405</td>\n",
       "      <td>0.012733</td>\n",
       "      <td>0.465494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM        ZN     INDUS     CHAS       NOX        RM       AGE  \\\n",
       "0  0.020938  0.001195  0.012092  0.04041  0.050353  0.197422  0.012369   \n",
       "\n",
       "        DIS      RAD       TAX   PTRATIO         B     LSTAT  \n",
       "0  0.050169  0.01311  0.039311  0.084405  0.012733  0.465494  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(regressor.feature_importances_.reshape(1, -1), columns=boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.713661449126906"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classification example\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:31:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 74.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\python-dscourse\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# First XGBoost model for Pima Indians dataset\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# load data\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# split data into train and test sets\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
